# ğŸ§  Ollama Local Chatbot  
### ğŸ¯ Run LLMs Locally with Python Terminal Interface ğŸš€


## ğŸ‘‹ Introduction

A lightweight, terminal-based chatbot built in Python that connects directly with locally hosted LLMs using the [Ollama](https://ollama.com) API. No internet required after model download â€” perfect for offline experiments and fast prototyping.

---

## ğŸŒ Live Demo / Repository Links

> ğŸ’» No live demo â€“ this is a local project  
> ğŸ“¦ [GitHub Repo](https://github.com/DishantBhere/local-chatbot)

---

## ğŸ“¸ Screenshots

âš ï¸ Note: Apologies for the image quality. Due to hardware limitations on my personal system, I was unable to run the model locally. The screenshots were taken using a college lab machine where the setup was successful.

![Screenshot 1](https://github.com/user-attachments/assets/005275ed-f93d-4a04-9101-3d24a49d2fe2) ![Screenshot 2](https://github.com/user-attachments/assets/14d08e2b-2e76-4864-a2bf-ec1da0cc3e4a) ![Screenshot 3](https://github.com/user-attachments/assets/c1c90c0b-1fb3-4957-bd96-ce9f9e2d3564)










---

## ğŸ“Œ Overview

This project allows you to run LLMs like `llama3`, `mistral`, or `gemma` on your machine through a simple Python script. It sends user input to Ollama's local server and returns model responses in the terminal.

---

## ğŸŒŸ Features

| Feature         | Description                                        |
|------------------|----------------------------------------------------|
| ğŸ§  Local AI       | No cloud required â€“ runs on your machine          |
| ğŸ’¬ Simple UI      | Just run from terminal, no fancy frontend         |
| âš¡ Fast Startup   | Lightweight and quick to load                     |
| ğŸ”„ Extensible     | Easy to modify or extend to other UIs or tools    |

---

## ğŸ§° Tech Stack

- ğŸ Python 3.x  
- ğŸ§  Ollama (Local LLM runtime)  
- ğŸ”— `requests` Python library  

---

## ğŸ“ Folder Structure
```
ollama-local-chatbot/
â”œâ”€â”€ main.py # Python script to interact with Ollama
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ requirements.txt # (Optional) list of dependencies
```

---

## ğŸš€ Getting Started

### 1. Install Ollama & Pull a Model

Install [Ollama](https://ollama.com) for your OS, then pull a model like:

```bash
ollama pull llama3
```
2. Clone the Repository
```
git clone https://github.com/yourusername/ollama-local-chatbot.git
cd ollama-local-chatbot

```
3. Run the Chatbot
```
python main.py
```
---

# ğŸ§   What I Learned
Using Ollama to run open-source LLMs locally

Building a minimal chat interface using Python

Handling local APIs and JSON responses

---

# ğŸš€ Future Improvements
 Add a basic web interface using Flask

 Multi-model switching

 Add markdown/text formatting to responses


### ğŸ‘¨â€ğŸ’» Made with ğŸ’› by:
 [DishantBhere](https://github.com/DishantBhere)  


---

![profile banner](https://user-images.githubusercontent.com/74038190/212284136-03988914-d899-44b4-b1d9-4eeccf656e44.gif)
